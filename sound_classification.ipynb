{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport shutil\n\nimport torch\nimport torchaudio\nfrom torch import nn\n!pip install torchmetrics\nimport torchvision.models as models\n\nfrom torchmetrics import Accuracy,Precision, Recall, F1Score,FBetaScore\nfrom torch.optim import Adam, AdamW, Adagrad,RMSprop,SGD\n\n!pip install torchvision\nimport torchvision.transforms as transform\n\nfrom torch.cuda.amp import autocast\n\n!pip install wget\n!pip install timm==0.4.5\n\nimport wget\nimport timm\n\nfrom timm.models.layers import to_2tuple,trunc_normal_\n\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom torch.utils.data import DataLoader\n\n\nfrom tqdm import tqdm\nimport numpy as np\n\nfrom IPython.display import FileLink\n\n\n%matplotlib inline\n%load_ext tensorboard\n\nimport librosa \nimport torch.nn.functional as Fx\nfrom torch.autograd import Variable\nimport sys\nfrom collections import OrderedDict","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"VAaizDLg_xTM","outputId":"97ad131a-56c4-48a5-cf5e-20f98799e946","execution":{"iopub.status.busy":"2022-09-15T06:29:16.594336Z","iopub.execute_input":"2022-09-15T06:29:16.594994Z","iopub.status.idle":"2022-09-15T06:29:55.595776Z","shell.execute_reply.started":"2022-09-15T06:29:16.594930Z","shell.execute_reply":"2022-09-15T06:29:55.594445Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.7/site-packages (0.9.3)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (4.3.0)\nRequirement already satisfied: torch>=1.3.1 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.11.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->torchmetrics) (3.0.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchvision) (4.3.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.11.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.28.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (9.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.21.6)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2022.6.15)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: wget in /opt/conda/lib/python3.7/site-packages (3.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: timm==0.4.5 in /opt/conda/lib/python3.7/site-packages (0.4.5)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.4.5) (0.12.0)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm==0.4.5) (1.11.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.4.5) (4.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.4.5) (2.28.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.4.5) (9.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.4.5) (1.21.6)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.4.5) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.4.5) (2022.6.15)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.4.5) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.4.5) (2.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n","output_type":"stream"}]},{"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive')\n#%cd /content/drive/MyDrive/ESC-50","metadata":{"id":"AbkCdYHzB8Vk","outputId":"fdf516b2-9cff-4cf3-ee9a-e83108f9e939","execution":{"iopub.status.busy":"2022-09-15T06:29:55.598031Z","iopub.execute_input":"2022-09-15T06:29:55.599070Z","iopub.status.idle":"2022-09-15T06:29:55.604102Z","shell.execute_reply.started":"2022-09-15T06:29:55.599023Z","shell.execute_reply":"2022-09-15T06:29:55.603092Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# пути и адреса\nPATHS = {\n    'to_audio':'../input/environmental-sound-classification-50/audio/audio/44100/', # путь к папке с аудиофайлами\n    'to_dataset': '../input/environmental-sound-classification-50/esc50.csv', # путь к .csv файлу\n    'to_tensors' : './tensors/', # Путь к папке, в которую сохраняются тензоры данных аудиофайлов\n    'to_original_tensors': './tensors/original/', # папка с оригинальными данными\n    'to_normalized_tensors': './tensors/normalized/', # папка с стандартизованными данными\n    'to_params': './params/',# папка с параметрами данных и модели\n    'to_z_score_params': './params/z_score', # папка с средними и дисперсиями, посчитанными попеременно по всем тренировочным fold\n    \n    'to_pretrained_weights_url':'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1', # ссылка на веса предобученной модели AST\n    'to_pretrained_model_weights':  './params/pretrained_model_weights.pth', # путь, по которому сохранятся указанные выше веса\n    \n    'to_trained_model_weights_url': './params/best_model_weights.pth', #'https://drive.google.com/file/d/1uSAeMslIhqJHK5k8cdOsdabRx6grSEmZ/view?usp=sharing',# ccылка на лучшие веса модели, достигнутые в процессе обучения\n    'to_trained_model_weights': './params/current_model_weights.pth',# путь, по которому сохранятся указанные выше веса\n    \n    'to_pretrained_cnn_weights': '../input/mypkl/mx-h64-1024_0d3-1.17.pkl'\n}\n\n# ниже установлены рекоммендуемые параметры для обучения модели\nCONFIG = {\n    # метки loss, metric, optimizer, scheduler имеют набор допустимых значений. Устанновив такое значение, вы создадите соответствующий объект внутри модели\n    'seed': 42,\n    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    'model': 'ast',#'ast' 'cnn', \n    'num_classes': 50,\n    \n    'metric': 'accuracy', # возможные значения:  accuracy, f1, recall, precision\n    'average': 'micro', #  возможные значения: micro, macro\n    'loss': 'logloss', # возможные значения: logloss\n    'epoches': 300,#2\n    'lr': 8e-6, #8e-6,\n    \n    'train_batch_size': 2,#2 # ВНИМАНИЕ! Не рекомендуется увеличивать значение этого параметра, если ваш GPU < 13GB. Используемае здесь модель занимает очень много места в памяти и обучать на больших батчах при небольшом GPU не получится.\n    'test_batch_size':80, #4,\n    \n    'feature_size': (1025, 216), # размерность представлений спектрограмм, с которыми будет работать модель\n    \n    'type_of_load_weights': 'pretrained',# веса, которые нужно загрузить\n    # pretrained - веса оригинальной модели AST, trained - лучшие веса модели, которые удалось получить за все время обучения\n    \n    # параметры распределения для z-стандартизации\n    'type_of_distribution_params': 'original', # допустимые значения: original, current. original - вычисленные на AudioSet для модели AST, current - вычисленные на текущем датасете\n    'original_mean': -4.2677393,\n    'original_std': 4.5689974,\n\n\n    #optimizer############\n    'optimizer': 'adam',# возможные значения: adam, adamw, sgd, rmsprop, adagrad\n    \n    'weight_decay': 0, # параметр регуляризации\n    'momentum': 1e-1, # моментум (только для SGD и RMSprop)\n    'smooth': 0.99,\n    'eps': 1e-8,# значение для численной стабильности знаменателя (только для RMSprop)\n    \n    # модификатор шага оптимайзера\n    #scheduler ###########\n    'scheduler': 'cosine', # возможные значения:  cosine, exponential, step\n    'lr_decay_factor': 0.96, # коэффициент, на который на каждом шаге умножается lr (только для exponential)\n    'step_size' : 200, # период снижения скорости обучения (только для step)\n    'min_lr': 1e-6, # # минимально возможный lr (только для cosine)\n    'max_iter': 2400#4800, # максимум итераций для работы алгоритма (только для cosine)\n}\n\n\n\n# Предусмотрена возможность добавления в модель пользовательских loss, metric, scheduler и optimizer\n# Формат добавления на примере метрики: {'metric': ('название метрики', класс_метрики, [список параметров в порядке их следования в объявлении метрики])}\n# Обратите внимение, что пользовательские инструменты \"перекрывают\" те, которые были объявлены в CONFIG\n\nCUSTOM_TOOLS = {\n    #'loss': ('mlsmloss', torch.nn.MultiLabelSoftMarginLoss, []),\n    #'metric': ('fbeta', FBetaScore, [CONFIG['num_classes'],0.5, 0.95,'macro']),\n    #'optimizer': ('radam', torch.optim.RAdam, [0.001,(0.9, 0.999),1e-08, 0]),\n    #'scheduler': ('lambdalr', torch.optim.lr_scheduler.LambdaLR, [lambda epoch: epoch // 30])\n}\n\nDATASET_CONFIG = {\n    'tfr': 'fbank'# Частотно-временные представления. Допустимые значения : mfcc, fbank\n}\n\n# создание необходимых папок\nif not os.path.exists(PATHS['to_tensors']):\n    os.mkdir(PATHS['to_tensors'])\n    os.mkdir(PATHS['to_original_tensors'])\n    os.mkdir(PATHS['to_normalized_tensors'])\nif not os.path.exists(PATHS['to_params']):\n    os.mkdir(PATHS['to_params'])\n    \ndef set_seed(seed=42):\n    '''\n    Фиксирование сида\n    '''\n    np.random.seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed(CONFIG['seed'])","metadata":{"id":"i6bJbQ61_xTT","execution":{"iopub.status.busy":"2022-09-15T06:29:55.605832Z","iopub.execute_input":"2022-09-15T06:29:55.606153Z","iopub.status.idle":"2022-09-15T06:29:55.623786Z","shell.execute_reply.started":"2022-09-15T06:29:55.606119Z","shell.execute_reply":"2022-09-15T06:29:55.622803Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"Изучим датасет. ","metadata":{"id":"WUTBHtSx0zAd"}},{"cell_type":"code","source":"features = pd.read_csv(PATHS['to_dataset'])\nfeatures","metadata":{"id":"VeGDD9gd1Tmi","outputId":"fe3f214c-3f9b-4b51-9e7f-23bdfe24cf62","execution":{"iopub.status.busy":"2022-09-15T06:29:55.628089Z","iopub.execute_input":"2022-09-15T06:29:55.628794Z","iopub.status.idle":"2022-09-15T06:29:55.662703Z","shell.execute_reply.started":"2022-09-15T06:29:55.628759Z","shell.execute_reply":"2022-09-15T06:29:55.661783Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"               filename  fold  target        category  esc10  src_file take\n0      1-100032-A-0.wav     1       0             dog   True    100032    A\n1     1-100038-A-14.wav     1      14  chirping_birds  False    100038    A\n2     1-100210-A-36.wav     1      36  vacuum_cleaner  False    100210    A\n3     1-100210-B-36.wav     1      36  vacuum_cleaner  False    100210    B\n4     1-101296-A-19.wav     1      19    thunderstorm  False    101296    A\n...                 ...   ...     ...             ...    ...       ...  ...\n1995   5-263831-B-6.wav     5       6             hen  False    263831    B\n1996  5-263902-A-36.wav     5      36  vacuum_cleaner  False    263902    A\n1997   5-51149-A-25.wav     5      25       footsteps  False     51149    A\n1998    5-61635-A-8.wav     5       8           sheep  False     61635    A\n1999     5-9032-A-0.wav     5       0             dog   True      9032    A\n\n[2000 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>fold</th>\n      <th>target</th>\n      <th>category</th>\n      <th>esc10</th>\n      <th>src_file</th>\n      <th>take</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1-100032-A-0.wav</td>\n      <td>1</td>\n      <td>0</td>\n      <td>dog</td>\n      <td>True</td>\n      <td>100032</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1-100038-A-14.wav</td>\n      <td>1</td>\n      <td>14</td>\n      <td>chirping_birds</td>\n      <td>False</td>\n      <td>100038</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1-100210-A-36.wav</td>\n      <td>1</td>\n      <td>36</td>\n      <td>vacuum_cleaner</td>\n      <td>False</td>\n      <td>100210</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1-100210-B-36.wav</td>\n      <td>1</td>\n      <td>36</td>\n      <td>vacuum_cleaner</td>\n      <td>False</td>\n      <td>100210</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1-101296-A-19.wav</td>\n      <td>1</td>\n      <td>19</td>\n      <td>thunderstorm</td>\n      <td>False</td>\n      <td>101296</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>5-263831-B-6.wav</td>\n      <td>5</td>\n      <td>6</td>\n      <td>hen</td>\n      <td>False</td>\n      <td>263831</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>5-263902-A-36.wav</td>\n      <td>5</td>\n      <td>36</td>\n      <td>vacuum_cleaner</td>\n      <td>False</td>\n      <td>263902</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>5-51149-A-25.wav</td>\n      <td>5</td>\n      <td>25</td>\n      <td>footsteps</td>\n      <td>False</td>\n      <td>51149</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>5-61635-A-8.wav</td>\n      <td>5</td>\n      <td>8</td>\n      <td>sheep</td>\n      <td>False</td>\n      <td>61635</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>5-9032-A-0.wav</td>\n      <td>5</td>\n      <td>0</td>\n      <td>dog</td>\n      <td>True</td>\n      <td>9032</td>\n      <td>A</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Из всех столбцов, полезными для обучения модели являются только 3: filename, fold, target.\n\nfilename - это путь к аудиофайлу.\n\nПокажем, что каждый filename в датасете уникален:","metadata":{"id":"5WeH467V1rQY"}},{"cell_type":"code","source":"print('Всего записей в датасете: ', features.shape[0])\nprint('Уникальных имен файлов: ', len(features.filename.unique()))","metadata":{"id":"UzctoA32M42K","outputId":"d6b80587-f5c3-48c4-9ca6-c66a7617f4cd","execution":{"iopub.status.busy":"2022-09-15T06:29:55.664039Z","iopub.execute_input":"2022-09-15T06:29:55.665071Z","iopub.status.idle":"2022-09-15T06:29:55.671862Z","shell.execute_reply.started":"2022-09-15T06:29:55.665035Z","shell.execute_reply":"2022-09-15T06:29:55.670736Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Всего записей в датасете:  2000\nУникальных имен файлов:  2000\n","output_type":"stream"}]},{"cell_type":"markdown","source":" Из этого файла при помощи библиотеки torchaudio следует извлечь данные mel-спектрограммы.","metadata":{"id":"F8Yu4332Nsx4"}},{"cell_type":"code","source":"filename = features.loc[1,:].filename \nwaveform, sr = torchaudio.load(PATHS['to_audio'] + filename)\n\nfeature_bank = torchaudio.compliance.kaldi.fbank(\n    waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n    window_type='hanning', num_mel_bins=216, dither=0.0,\n    frame_shift=10)\n\nfeature_bank","metadata":{"id":"V7XMGk8r3mv6","outputId":"25e73662-1a6b-42a6-c0a2-fba5118d8543","execution":{"iopub.status.busy":"2022-09-15T06:29:55.673617Z","iopub.execute_input":"2022-09-15T06:29:55.674917Z","iopub.status.idle":"2022-09-15T06:29:55.711476Z","shell.execute_reply.started":"2022-09-15T06:29:55.674881Z","shell.execute_reply":"2022-09-15T06:29:55.710479Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"tensor([[-12.5984, -11.8655, -15.9424,  ..., -14.0511, -13.2287, -12.6782],\n        [-11.1301, -10.3245, -14.0157,  ..., -13.4380, -13.0887, -12.9269],\n        [-15.9424, -15.9424, -15.9424,  ..., -13.2381, -13.5167, -13.3581],\n        ...,\n        [-12.9755, -12.3088, -15.9424,  ..., -12.6964, -13.1629, -12.7698],\n        [-15.9424, -15.7064, -15.9424,  ..., -13.3250, -12.8344, -12.9705],\n        [-15.9424, -15.0021, -15.9424,  ..., -13.4089, -13.1005, -13.9848]])"},"metadata":{}}]},{"cell_type":"markdown","source":"target - номер метки класса, которому сопоставляется аудиофайл.","metadata":{"id":"SMMfZQKQ4Bu7"}},{"cell_type":"code","source":"features.target.value_counts()","metadata":{"id":"HE9yMuBOpOiQ","outputId":"6efbc58e-2371-426b-ce8a-8d915c227b8e","execution":{"iopub.status.busy":"2022-09-15T06:29:55.713689Z","iopub.execute_input":"2022-09-15T06:29:55.714590Z","iopub.status.idle":"2022-09-15T06:29:55.723800Z","shell.execute_reply.started":"2022-09-15T06:29:55.714545Z","shell.execute_reply":"2022-09-15T06:29:55.722817Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"0     40\n39    40\n29    40\n10    40\n7     40\n26    40\n6     40\n44    40\n23    40\n20    40\n49    40\n24    40\n28    40\n14    40\n18    40\n2     40\n35    40\n38    40\n21    40\n1     40\n11    40\n42    40\n5     40\n33    40\n40    40\n12    40\n43    40\n27    40\n36    40\n19    40\n30    40\n34    40\n9     40\n22    40\n48    40\n41    40\n47    40\n31    40\n17    40\n45    40\n8     40\n15    40\n46    40\n37    40\n32    40\n16    40\n25    40\n4     40\n3     40\n13    40\nName: target, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Видно, что каждого класса в датасете одинаковое количество. Значит, датасет сбалансирован.","metadata":{"id":"3HHT45Um4gBW"}},{"cell_type":"markdown","source":"fold - папки, по которым разбит датасет для удобства обучения.","metadata":{"id":"j5-TuVGC4xnN"}},{"cell_type":"code","source":"res = []\nfor i in range(features.fold.max()):\n  ds_ = features[features.fold == i+1]\n  res.append(ds_.target.value_counts())\n  \npd.DataFrame(res)","metadata":{"id":"D--mVTqE4Nc9","outputId":"ee060dd5-2065-43ae-f1fc-6126c28ea201","execution":{"iopub.status.busy":"2022-09-15T06:29:55.725446Z","iopub.execute_input":"2022-09-15T06:29:55.726190Z","iopub.status.idle":"2022-09-15T06:29:55.753681Z","shell.execute_reply.started":"2022-09-15T06:29:55.726153Z","shell.execute_reply":"2022-09-15T06:29:55.752538Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"        0   1   2   3   4   5   6   7   8   9   ...  40  41  42  43  44  45  \\\ntarget   8   8   8   8   8   8   8   8   8   8  ...   8   8   8   8   8   8   \ntarget   8   8   8   8   8   8   8   8   8   8  ...   8   8   8   8   8   8   \ntarget   8   8   8   8   8   8   8   8   8   8  ...   8   8   8   8   8   8   \ntarget   8   8   8   8   8   8   8   8   8   8  ...   8   8   8   8   8   8   \ntarget   8   8   8   8   8   8   8   8   8   8  ...   8   8   8   8   8   8   \n\n        46  47  48  49  \ntarget   8   8   8   8  \ntarget   8   8   8   8  \ntarget   8   8   8   8  \ntarget   8   8   8   8  \ntarget   8   8   8   8  \n\n[5 rows x 50 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>40</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>target</th>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>...</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>target</th>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>...</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>target</th>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>...</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>target</th>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>...</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>target</th>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>...</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 50 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Видно, что в каждой папке находится одинаковое количество объектов каждого класса.","metadata":{"id":"po8SzLlt6QtW"}},{"cell_type":"markdown","source":"На основе описанных выше наблюдений можно сделать вывод, что в качестве главной метрики для модели-классификатора можно выбрать accuracy.","metadata":{"id":"VY5yyv6KH31u"}},{"cell_type":"code","source":"class PreprocessorESC50:\n    '''\n    Предобработчик данных. Удаляет лишние данные. Сохраняет данные в виде тензоров, вычисляет параметры распределения по тренировочным папкам \n    '''\n    def __init__(self,CONFIG, DATASET_CONFIG, PATHS):\n        self.__path_to_dataset = PATHS['to_dataset']\n        self.__path_to_audio =  PATHS['to_audio']\n        self.__path_to_orig_tensors = PATHS['to_original_tensors']\n        self.__path_to_norm_tensors = PATHS['to_normalized_tensors']\n        self.__path_to_params = PATHS['to_params']\n        self.__path_to_z_score_params = PATHS['to_z_score_params']\n        \n        self.__feature_size = CONFIG['feature_size']\n        \n        self.features  = pd.read_csv(self.__path_to_dataset)\n        self.target = self.features['target']\n        \n        self.tfr = DATASET_CONFIG['tfr']\n        \n    def __create_feature_bank(self, wav_name, mel_bins, target_length):\n        '''\n        Извлекает из аудиофрагмента набор данных,полученных с помощью mel-спектрограммы\n        \n        wav_name - имя файла\n        mel_bins - количество частотных ячеек\n        target_length - размерность выходных данных\n        '''\n        waveform, sr = torchaudio.load(wav_name)\n\n        feature_bank = torchaudio.compliance.kaldi.fbank(\n            waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n            window_type='hanning', num_mel_bins=mel_bins, dither=0.0,\n            frame_shift=10)\n\n        n_frames = feature_bank.shape[0]\n\n        difference = target_length - n_frames\n        if difference > 0:# Если до размерности выходных данных немного не хватает, то дозаполним данные нулями\n            pad = torch.nn.ZeroPad2d((0, 0, 0, difference)) \n            feature_bank = pad(feature_bank)\n        elif difference < 0: # Если данных слишком много - обрежем их\n            feature_bank = feature_bank[0:target_length, :]\n\n        return feature_bank\n    \n    def __create_mfcc(self, wav_name, num_seps, target_length):\n        waveform, sr = torchaudio.load(wav_name)\n        feature = torchaudio.compliance.kaldi.mfcc(waveform, htk_compat=True,num_ceps=num_seps, sample_frequency=sr, use_energy=False,\n            window_type='hanning', num_mel_bins=num_seps, dither=0.0,\n            frame_shift=10)\n        n_frames = feature.shape[0]\n\n        difference = target_length - n_frames\n        if difference > 0:# Если до размерности выходных данных немного не хватает, то дозаполним данные нулями\n            pad = torch.nn.ZeroPad2d((0, 0, 0, difference)) \n            feature = pad(feature)\n        elif difference < 0: # Если данных слишком много - обрежем их\n            feature = feature[0:target_length, :]\n        return feature\n        \n    def __create_and_save_tensors(self, row):\n        '''\n        Загружает файл, извлекает и сохраняет признаки и целевые переменные в виде тензоров\n        row - запись в датасете\n        '''\n        filename = row['filename']\n        idx = row.name \n        path = ''.join([self.__path_to_audio, filename]) # место сохранения тензора\n        \n        if self.tfr == 'fbank':\n            feature = self.__create_feature_bank(path, self.__feature_size[1], self.__feature_size[0])\n        elif self.tfr == 'mfcc':\n            feature = self.__create_mfcc(path, self.__feature_size[1], self.__feature_size[0])\n        \n\n        path_to_feature = ''.join([self.__path_to_orig_tensors, filename]) \n        path_to_target = ''.join([self.__path_to_orig_tensors, filename, '_target']) \n        features_tensor = torch.Tensor(feature)\n\n        target_tensor = torch.Tensor(self.target[idx])\n\n        torch.save(features_tensor, path_to_feature)\n        torch.save(target_tensor, path_to_target)\n        return path_to_feature\n    \n    def __compute_z_score_params(self):\n        '''\n        Метод подсчета параметров распределения для z-стандартизации\n        '''\n        max_num_fold = self.features['fold'].max()\n        counts = np.zeros((max_num_fold,))\n        means = np.zeros((max_num_fold,))\n        stds = np.zeros((max_num_fold,))\n\n        for i, data in self.features.iterrows():\n            tensor = torch.load(''.join([self.__path_to_orig_tensors, data.filename])) \n            mean = torch.mean(tensor) # среднее по тензору\n            std = torch.std(tensor) # стандартное отклонение по тензору\n            for j in range(0, max_num_fold): # Считаем параметры распределения по всем комбинациям из 4 тренировочных папок\n                if j + 1 != data.fold:\n                    stds[j] += std\n                    means[j] += mean\n                    counts[j] += 1\n        means /= counts \n        stds /= counts  \n\n        self.__z_params = np.concatenate([means.reshape(1, -1), stds.reshape(1, -1)], axis=0)\n        torch.save(torch.Tensor(self.__z_params), self.__path_to_z_score_params) # сохраняем параметры распределения\n\n    def preprocess(self):\n        '''\n        Основной метод предобработки данных\n        '''\n        self.ohe = OneHotEncoder(sparse=False)\n        self.target = self.ohe.fit_transform(self.target.to_numpy().reshape(-1, 1)) # представляем target в виде one-hot векторов\n        _ = self.features.apply(self.__create_and_save_tensors, axis=1) # создаем и сохраняем тензоры\n        self.features.drop(labels = ['target', 'category', 'esc10', 'src_file', 'take'], axis=1, inplace=True) # удаляем ненужные данные\n        self.__compute_z_score_params() # вычисляем и сохраняем параметры распределения\n        return self.features, self.target","metadata":{"id":"vxmbxzKc_xTV","execution":{"iopub.status.busy":"2022-09-15T06:29:55.755615Z","iopub.execute_input":"2022-09-15T06:29:55.756295Z","iopub.status.idle":"2022-09-15T06:29:55.780126Z","shell.execute_reply.started":"2022-09-15T06:29:55.756259Z","shell.execute_reply":"2022-09-15T06:29:55.779156Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"dataset = PreprocessorESC50(CONFIG,DATASET_CONFIG, PATHS)\ndataset.preprocess()  # ВНИМАНИЕ! Данные метод стоит выполнять только при первом запуске ноутбука. Все результаты его работы будут сохранены, и сам метод\n# можно будет закомментировать. Это существенно повысит скорость выполнения ноутбука","metadata":{"id":"QTkk298n2lq5","outputId":"e013bd36-f0eb-429d-89bd-036b808ca241","execution":{"iopub.status.busy":"2022-09-15T06:29:55.783919Z","iopub.execute_input":"2022-09-15T06:29:55.784179Z","iopub.status.idle":"2022-09-15T06:30:31.111586Z","shell.execute_reply.started":"2022-09-15T06:29:55.784155Z","shell.execute_reply":"2022-09-15T06:30:31.110533Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"(               filename  fold\n 0      1-100032-A-0.wav     1\n 1     1-100038-A-14.wav     1\n 2     1-100210-A-36.wav     1\n 3     1-100210-B-36.wav     1\n 4     1-101296-A-19.wav     1\n ...                 ...   ...\n 1995   5-263831-B-6.wav     5\n 1996  5-263902-A-36.wav     5\n 1997   5-51149-A-25.wav     5\n 1998    5-61635-A-8.wav     5\n 1999     5-9032-A-0.wav     5\n \n [2000 rows x 2 columns],\n array([[1., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [1., 0., 0., ..., 0., 0., 0.]]))"},"metadata":{}}]},{"cell_type":"markdown","source":"Выведем параметры распределения по каждой папке:","metadata":{"id":"Ir6y-HOs5UTc"}},{"cell_type":"code","source":"params = torch.load(PATHS['to_z_score_params'])\nd = {'mean': params[0], 'std': params[1]}\npd.DataFrame(data=d, index = np.arange(1, 6))","metadata":{"id":"g9Y4-9G95TSl","outputId":"641f3f91-b40d-46d4-d727-9087fa070ffc","execution":{"iopub.status.busy":"2022-09-15T06:30:31.113347Z","iopub.execute_input":"2022-09-15T06:30:31.113913Z","iopub.status.idle":"2022-09-15T06:30:31.127671Z","shell.execute_reply.started":"2022-09-15T06:30:31.113873Z","shell.execute_reply":"2022-09-15T06:30:31.126479Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"       mean       std\n1 -3.234985  4.445635\n2 -3.240178  4.444564\n3 -3.208234  4.438404\n4 -3.238236  4.453884\n5 -3.238322  4.461284","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>-3.234985</td>\n      <td>4.445635</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-3.240178</td>\n      <td>4.444564</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-3.208234</td>\n      <td>4.438404</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-3.238236</td>\n      <td>4.453884</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-3.238322</td>\n      <td>4.461284</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Как можно заметить, распределение данных по всем папкам приблизительно одинаковое.","metadata":{"id":"3S0cHc6S9OLr"}},{"cell_type":"code","source":"class DatasetECS50:\n    '''\n    Класс предобработанного датасета, с возможностью итерации по нему.\n    Реализованы z-стандартизации,\n    а также метод разбиения на тренировочный и тестовый датасет.\n    '''\n    def __init__(self, PATHS, features, target):\n        self.__path_to_dataset = PATHS['to_dataset']\n        self.__path_to_audio =  PATHS['to_audio']\n        self.__path_to_orig_tensors = PATHS['to_original_tensors']\n        self.__path_to_norm_tensors = PATHS['to_normalized_tensors']\n        self.__path_to_params = PATHS['to_params']\n        self.__path_to_z_score_params = PATHS['to_z_score_params']\n        self.features = features\n        self.target = target\n\n        self.type_of_dist_params = CONFIG['type_of_distribution_params']\n        self.original_mean = CONFIG['original_mean']\n        self.original_std = CONFIG['original_std']\n\n    def __len__(self):\n        '''\n        Метод, необходимый для возможности итерирования по датасету\n        '''\n        return self.features.shape[0]\n    \n    def train_test_split(self, test_fold_num):\n        '''\n        Разбиение датасетов на тестовый и тренировочный по номеру папки. Возвращает эти самые датасеты\n        '''\n        self.cure_fold = test_fold_num # фиксируем тренировочную папку\n        train = self.features[self.features.fold != test_fold_num].reset_index() # выбираем все данные, соответствующие тренировочным папкам\n        test = self.features[self.features.fold == test_fold_num].reset_index() # выбираем данные, соответствующие тестовой папке\n        train['filename'].map(self.__normalize_and_save) # стандартизуем, для более быстрой сходимости алгоритма \n        test['filename'].map(self.__normalize_and_save)\n        \n        train_index = train.set_index('index').index\n        test_index = test.set_index('index').index\n        train.drop(labels = ['index'], axis=1, inplace=True)\n        test.drop(labels = ['index'], axis=1, inplace=True)\n        \n        return DatasetECS50(PATHS, train, self.target[train_index]), DatasetECS50(PATHS, test, self.target[test_index])\n\n    def __apply_z_score(self, filename):   \n        '''\n        Метод применения z-стандартизации к тензору\n        '''\n        tensor = torch.load(''.join([self.__path_to_orig_tensors, filename]))\n        \n        fold_of_cur_record = self.cure_fold\n\n        if self.type_of_dist_params == 'original': # стандартизация с помощью параметров распределения модели на датасете AudioSet, на котором она предобучалась \n            tensor = (tensor.view(1, tensor.shape[0], tensor.shape[1]) - (self.original_mean)) / (self.original_std * 2)\n        elif self.type_of_dist_params == 'current':\n            z_score_params = torch.load(self.__path_to_z_score_params) # стандартизация с помащью параметров распределения этого датасета\n            tfm = transform.Normalize(mean=z_score_params[0][fold_of_cur_record - 1], std=z_score_params[1][fold_of_cur_record - 1])\n            tensor = tfm(tensor.view(1, tensor.shape[0], tensor.shape[1]))\n        return tensor\n \n    def __normalize_and_save(self, filename):\n        '''\n        Метод нормализации и сохранения тензоров\n        '''\n        tensor = self.__apply_z_score(filename)\n        path_to_feature = ''.join([self.__path_to_norm_tensors, filename])\n        torch.save(tensor, path_to_feature)\n        \n    def __getitem__(self, ids):\n        '''\n        Метод, необходимый для возможности итерирования по датасету\n        Возвращает пару (данные, целевая переменная)\n        '''\n        filename = self.features.loc[ids,:].filename\n        tensor = torch.load(''.join([self.__path_to_norm_tensors, filename]))\n        tensor = tensor.view(tensor.shape[1], tensor.shape[2])\n        target_tensor = torch.load(''.join([self.__path_to_orig_tensors, filename, '_target']))\n        \n        return (tensor, target_tensor) ","metadata":{"id":"PgnEWhal_xTY","execution":{"iopub.status.busy":"2022-09-15T06:30:31.129093Z","iopub.execute_input":"2022-09-15T06:30:31.129640Z","iopub.status.idle":"2022-09-15T06:30:31.147910Z","shell.execute_reply.started":"2022-09-15T06:30:31.129603Z","shell.execute_reply":"2022-09-15T06:30:31.146900Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"ds = DatasetECS50(PATHS , dataset.features, dataset.target)\ntrain, test = ds.train_test_split(test_fold_num=5)\ntrain.features","metadata":{"id":"PuMzvoiy2ogI","outputId":"6fa9f145-c39f-4cc4-a5e7-8f758a24c0a2","execution":{"iopub.status.busy":"2022-09-15T06:30:31.151054Z","iopub.execute_input":"2022-09-15T06:30:31.151346Z","iopub.status.idle":"2022-09-15T06:30:42.146440Z","shell.execute_reply.started":"2022-09-15T06:30:31.151321Z","shell.execute_reply":"2022-09-15T06:30:42.145539Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"               filename  fold\n0      1-100032-A-0.wav     1\n1     1-100038-A-14.wav     1\n2     1-100210-A-36.wav     1\n3     1-100210-B-36.wav     1\n4     1-101296-A-19.wav     1\n...                 ...   ...\n1595    4-99193-B-4.wav     4\n1596    4-99644-A-4.wav     4\n1597    4-99644-B-4.wav     4\n1598    4-99644-C-4.wav     4\n1599    4-99644-D-4.wav     4\n\n[1600 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1-100032-A-0.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1-100038-A-14.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1-100210-A-36.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1-100210-B-36.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1-101296-A-19.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>4-99193-B-4.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>4-99644-A-4.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1597</th>\n      <td>4-99644-B-4.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1598</th>\n      <td>4-99644-C-4.wav</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1599</th>\n      <td>4-99644-D-4.wav</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>1600 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"В процессе выбора модели, я пришел к выводу, что данная задача плохо решается с помощью обычных RNN и CNN, а действительно хороших результатов можно достичь только с помощью архитектур Transformer или подобных им.\n\nИменно поэтому свой выбор модели я остановил на Audio Spectrogram Transformer (AST), предобученный на датасетах ImageNet и AudioSet, взятый практически без изменений из открытого доступа. Данная модель показыват одни из самых лучших результатов при работе с аудиофрагментами. \n\nЭта модель практически не имеет недостатков. Из серьезных минусов можно выделить только то, что она довольно тяжеловесная, и если загружать ее стандартного размера GPU Kaggle или Colab, то придется обучать модель небольшими батчами данных, т.к. большую часть памяти GPU будет занимать именно модель.\n\nКод AST я практически без изменений оставил в ячейке ниже.","metadata":{"id":"djOGE5s59LOs"}},{"cell_type":"code","source":"class PatchEmbed(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\nclass ASTModel(nn.Module):\n    '''\n    Audio Spectrogram Transformer, предобученный на датасетах ImageNet и AudioSet, взятый практически без изменений из открытого доступа.\n    '''\n    def __init__(self,PATHS, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=True, audioset_pretrain=False, model_size='base384', verbose=False):\n        \n        super(ASTModel, self).__init__()\n        assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n        \n        if verbose == True:\n            print('---------------AST Model Summary---------------')\n            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n\n        if not audioset_pretrain:\n            if model_size == 'tiny224':\n                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n            elif model_size == 'small224':\n                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n            elif model_size == 'base224':\n                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n            elif model_size == 'base384':\n                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n            else:\n                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n                \n            self.original_num_patches = self.v.patch_embed.num_patches\n            self.oringal_hw = int(self.original_num_patches ** 0.5) \n            self.original_embedding_dim = self.v.pos_embed.shape[2]\n            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n\n            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n            num_patches = f_dim * t_dim\n            self.v.patch_embed.num_patches = num_patches\n            if verbose == True:\n                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n                print('number of patches={:d}'.format(num_patches))\n\n            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n            if imagenet_pretrain == True:\n                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n                new_proj.bias = self.v.patch_embed.proj.bias\n            self.v.patch_embed.proj = new_proj\n\n            if imagenet_pretrain == True:\n                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n                if t_dim <= self.oringal_hw:\n                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw / 2) - int(t_dim / 2): int(self.oringal_hw / 2) - int(t_dim / 2) + t_dim]\n                else:\n                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n                if f_dim <= self.oringal_hw:\n                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw / 2) - int(f_dim / 2): int(self.oringal_hw / 2) - int(f_dim / 2) + f_dim, :]\n                else:\n                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n            else:\n                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n                self.v.pos_embed = new_pos_embed\n                trunc_normal_(self.v.pos_embed, std=.02)\n\n        elif audioset_pretrain:\n            if audioset_pretrain and not imagenet_pretrain:\n                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n            if model_size != 'base384':\n                raise ValueError('currently only has base384 AudioSet pretrained model.')\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n            self.audio_model = ASTModel(PATHS, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n            self.audio_model = torch.nn.DataParallel(self.audio_model)\n            self.pretrained_weights = PATHS['to_pretrained_model_weights']\n            self.pretrained_weights_url = PATHS['to_pretrained_weights_url']\n            if not os.path.exists(self.pretrained_weights):\n                wget.download(self.pretrained_weights_url, out=self.pretrained_weights)\n            self.set_pretrained_weights()\n            self.v = self.audio_model.module.v\n            self.original_embedding_dim = self.v.pos_embed.shape[2]\n            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n\n            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n            num_patches = f_dim * t_dim\n            self.v.patch_embed.num_patches = num_patches\n            if verbose:\n                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n                print('number of patches={:d}'.format(num_patches))\n\n            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n            if t_dim < 101:\n                new_pos_embed = new_pos_embed[:, :, :, 50 - int(t_dim/2): 50 - int(t_dim/2) + t_dim]\n            else:\n                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(12, t_dim), mode='bilinear')\n            if f_dim < 12:\n                new_pos_embed = new_pos_embed[:, :, 6 - int(f_dim/2): 6 - int(f_dim/2) + f_dim, :]\n            elif f_dim > 12:\n                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n            new_pos_embed = new_pos_embed.reshape(1, 768, num_patches).transpose(1, 2)\n            self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n\n    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n        test_out = test_proj(test_input)\n        f_dim = test_out.shape[2]\n        t_dim = test_out.shape[3]\n        return f_dim, t_dim\n\n    def set_pretrained_weights(self):\n        state_dict = torch.load(self.pretrained_weights, map_location=self.device)\n        self.audio_model.load_state_dict(state_dict, strict=False)\n    \n\n    @autocast()\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = x.transpose(2, 3)\n\n        B = x.shape[0]\n        x = self.v.patch_embed(x)\n        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n        dist_token = self.v.dist_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n        x = x + self.v.pos_embed\n        x = self.v.pos_drop(x)\n        for blk in self.v.blocks:\n            x = blk(x)\n        x = self.v.norm(x)\n        x = (x[:, 0] + x[:, 1]) / 2\n\n        x = self.mlp_head(x)\n        return x","metadata":{"id":"Id2_x_1v_xTZ","execution":{"iopub.status.busy":"2022-09-15T06:30:42.148017Z","iopub.execute_input":"2022-09-15T06:30:42.148752Z","iopub.status.idle":"2022-09-15T06:30:42.186682Z","shell.execute_reply.started":"2022-09-15T06:30:42.148710Z","shell.execute_reply":"2022-09-15T06:30:42.185721Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, nclass):\n        super(CNN,self).__init__() \n        self.globalpool = Fx.avg_pool2d\n        self.layer1 = nn.Sequential(nn.Conv2d(1,16,kernel_size=3,padding=1),nn.BatchNorm2d(16),nn.ReLU())\n        self.layer2 = nn.Sequential(nn.Conv2d(16,16,kernel_size=3,padding=1),nn.BatchNorm2d(16),nn.ReLU())\n        self.layer3 = nn.MaxPool2d(2)\n\n        self.layer4 = nn.Sequential(nn.Conv2d(16,32,kernel_size=3,padding=1),nn.BatchNorm2d(32),nn.ReLU())\n        self.layer5 = nn.Sequential(nn.Conv2d(32,32,kernel_size=3,padding=1),nn.BatchNorm2d(32),nn.ReLU())\n        self.layer6 = nn.MaxPool2d(2)\n\n        self.layer7 = nn.Sequential(nn.Conv2d(32,64,kernel_size=3,padding=1),nn.BatchNorm2d(64),nn.ReLU())\n        self.layer8 = nn.Sequential(nn.Conv2d(64,64,kernel_size=3,padding=1),nn.BatchNorm2d(64),nn.ReLU())\n        self.layer9 = nn.MaxPool2d(2)\n\n        self.layer10 = nn.Sequential(nn.Conv2d(64,128,kernel_size=3,padding=1),nn.BatchNorm2d(128),nn.ReLU())\n        self.layer11 = nn.Sequential(nn.Conv2d(128,128,kernel_size=3,padding=1),nn.BatchNorm2d(128),nn.ReLU())\n        self.layer12 = nn.MaxPool2d(2)\n\n        self.layer13 = nn.Sequential(nn.Conv2d(128,256,kernel_size=3,padding=1),nn.BatchNorm2d(256),nn.ReLU())\n        self.layer14 = nn.Sequential(nn.Conv2d(256,256,kernel_size=3,padding=1),nn.BatchNorm2d(256),nn.ReLU())\n        self.layer15 = nn.MaxPool2d(2) #\n\n        self.layer16 = nn.Sequential(nn.Conv2d(256,512,kernel_size=3,padding=1),nn.BatchNorm2d(512),nn.ReLU())\n        self.layer17 = nn.MaxPool2d(2) # \n        \n        self.layer18 = nn.Sequential(nn.Conv2d(512,1024,kernel_size=2),nn.BatchNorm2d(1024),nn.ReLU())\n        self.layer19 = nn.Sequential(nn.Conv2d(1024,nclass,kernel_size=1))\n\n    def forward(self,x):\n        x = x.reshape(x.shape[0], -1, x.shape[1],x.shape[2])\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = self.layer6(out)\n        out = self.layer7(out)\n        out = self.layer8(out)\n        out = self.layer9(out)\n        out = self.layer10(out)\n        out = self.layer11(out)\n        out = self.layer12(out)\n        out = self.layer13(out)\n        out = self.layer14(out)\n        out = self.layer15(out)\n        out = self.layer16(out)\n        out = self.layer17(out)\n        out = self.layer18(out)\n        out1 = self.layer19(out) \n        out = self.globalpool(out1,kernel_size=out1.size()[2:])\n        out = out.view(out.size(0),-1)\n        return out #,out1","metadata":{"execution":{"iopub.status.busy":"2022-09-15T06:30:42.188301Z","iopub.execute_input":"2022-09-15T06:30:42.188738Z","iopub.status.idle":"2022-09-15T06:30:42.208173Z","shell.execute_reply.started":"2022-09-15T06:30:42.188702Z","shell.execute_reply":"2022-09-15T06:30:42.207149Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"class AudioClassifier:\n    '''\n    Модель для задачи классификации датасета ESC-50. Имеет возможность удобной настройки параметров через CONFIG. Для изменения объектов обучения - \n    loss, optimizer, scheduler, metric - достаточно просто подать правильную метку в CONFIG.\n    Реализованы методы обычного обучения и кросс-валидации. \n\n    На каждом этапе валидации, при достижении более высокого результата метрики, веса модели сохраняются. Их можно скачать с помощью метода download weights\n    '''\n    def __init__(self, CONFIG, PATHS, CUSTOM_TOOLS):\n        ##### общие характеристики #####\n        self.device = CONFIG['device']\n        self.num_classes = CONFIG['num_classes']\n        self.model_name = CONFIG['model']\n        self.lr = CONFIG['lr']\n        self.epoches = CONFIG['epoches']\n        self.train_batch_size = CONFIG['train_batch_size']\n        self.test_batch_size = CONFIG['test_batch_size']\n        self.path_to_trained_weights = PATHS['to_trained_model_weights'] # путь к лучшим весам этой модели, которые удалось получить за все время обучения\n        self.__feature_size = CONFIG['feature_size'] # требуемая размерность WxH данных\n        \n        self.type_of_load_weights = CONFIG['type_of_load_weights']\n        \n        self.path_to_pretrained_cnn_weights = PATHS['to_pretrained_cnn_weights']\n        \n        ##### модель #####\n        if self.model_name == 'ast':\n            self.model = ASTModel(\n                PATHS,\n                input_tdim =self.__feature_size[0],\n                input_fdim=self.__feature_size[1],\n                label_dim=self.num_classes,\n                audioset_pretrain=True,\n            ).to(self.device)\n            # веса для AST при значении параметра type_of_load_weights == 'pretrained' скачиваются при инициализации AST\n            # при значении trained - скачиваются наилучшие веса, полученные в процессе обучения\n            if self.type_of_load_weights == 'trained':\n                self.path_to_trained_weights_url = PATHS['to_trained_model_weights_url']\n                if not os.path.exists(self.path_to_trained_weights): # если по указанному адресу ничего нет - скачиваем туда веса классификатора.\n                    wget.download(self.path_to_trained_weights_url, out=self.path_to_trained_weights)                                                                    \n                self.load_weights(self.path_to_trained_weights)\n        elif self.model_name == 'cnn':\n            self.model = CNN(self.num_classes).to(self.device)\n            self.load_weights(self.path_to_pretrained_cnn_weights, model_name='cnn')\n\n        self.softmax = nn.Softmax(dim=1)\n\n        ##### оптимизаторы #####\n        if CUSTOM_TOOLS.get('optimizer') is not None: # если пользователь подал кастомный оптимизатор, то выбираем именно его, иначе тот, который указана в конфиге\n            self.optimizer_name = CUSTOM_TOOLS['optimizer'][0]\n            self.optimizer = CUSTOM_TOOLS['optimizer'][1](self.model.parameters(), *CUSTOM_TOOLS['optimizer'][2])\n        else:\n            self.optimizer_name = CONFIG['optimizer']\n            self.weight_decay = CONFIG['weight_decay']\n            \n            if self.optimizer_name == 'adamw':\n                self.optimizer = AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n            elif self.optimizer_name == 'adam':\n                self.optimizer = Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n            elif self.optimizer_name == 'sgd':\n                self.momentum = CONFIG['momentum']\n                self.optimizer = SGD(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay,momentum=self.momentum) \n            elif self.optimizer_name == 'adagrad':\n                self.optimizer = Adagrad(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n            elif self.optimizer_name == 'rmsprop':\n                self.momentum = CONFIG['momentum']\n                self.smooth = CONFIG['smooth']\n                self.eps = CONFIG['smooth']\n                self.optimizer = RMSprop(\n                    self.model.parameters(),\n                    lr=self.lr,\n                    alpha=self.smooth,\n                    eps=self.eps,\n                    weight_decay=self.weight_decay, \n                    momentum=self.momentum\n                )\n            self.type_of_optimizers = {'adamw': AdamW, 'adam': Adam, 'sgd': SGD, 'adagrad': Adagrad, 'rmsprop':RMSprop}\n\n        ##### метрики #####\n\n        if CUSTOM_TOOLS.get('metric') is not None:# если пользователь подал кастомную метрику, то выбираем именно ее, иначе ту, который указана в конфиге\n            self.metric_name = CUSTOM_TOOLS['metric'][0]\n            self.metric = CUSTOM_TOOLS['metric'][1](*CUSTOM_TOOLS['metric'][2])\n        else:\n            self.metric_name = CONFIG['metric']\n            self.metric_average = CONFIG['average']\n\n            if self.metric_name == 'accuracy':\n                self.metric = Accuracy(average=self.metric_average, num_classes=self.num_classes)\n            elif self.metric_name == 'f1':\n                self.metric = F1Score(average=self.metric_average,num_classes=self.num_classes) \n            elif self.metric_name == 'precision':\n                self.metric = Precision(average=self.metric_average,num_classes=self.num_classes)\n            elif self.metric_name == 'recall':\n                self.metric = Recall(average=self.metric_average,num_classes=self.num_classes)\n            \n            \n        ##### функции потерь #####\n        if CUSTOM_TOOLS.get('loss') is not None: # если пользователь подал кастомный лосс, то выбираем именно его, иначе тот, который указана в конфиге\n            self.criterion_name = CUSTOM_TOOLS['loss'][0]\n            self.criterion = CUSTOM_TOOLS['loss'][1](*CUSTOM_TOOLS['loss'][2]).to(self.device)\n        else:\n            self.criterion_name = CONFIG['loss']\n            if self.criterion_name == 'logloss':\n                self.criterion = nn.CrossEntropyLoss().to(self.device) \n\n        \n        \n        ##### модификаторы для шага #####\n        if CUSTOM_TOOLS.get('scheduler') is not None: # если пользователь подал кастомный scheduler, то выбираем именно его, иначе тот, который указана в конфиге\n            self.scheduler_name = CUSTOM_TOOLS['scheduler'][0]\n            self.scheduler = CUSTOM_TOOLS['scheduler'][1](self.optimizer, *CUSTOM_TOOLS['scheduler'][2])\n        else:\n            self.max_iter = CONFIG['max_iter']\n            self.scheduler_name = CONFIG['scheduler']\n            self.min_lr = CONFIG['min_lr']\n\n            if self.scheduler_name == 'cosine':\n                self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer,T_max=self.max_iter,  eta_min=self.min_lr, verbose=False)\n            elif self.scheduler_name == 'exponential':\n                self.lr_decay_factor = CONFIG['lr_decay_factor']\n                self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=self.lr_decay_factor, verbose=False)\n            elif self.scheduler_name == 'step':\n                self.lr_decay_factor = CONFIG['lr_decay_factor']\n                self.step_size = CONFIG['step_size']\n                self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=self.step_size, gamma=self.lr_decay_factor,verbose=False)\n            \n    def save_weights(self, path='./start_weigths'):\n        '''\n        Метод сохранения весов модели по указанному адресу\n        '''\n        if os.path.exists(path):\n            os.remove(path)\n        torch.save(self.model.state_dict(), path)\n\n    def load_weights(self, path='./start_weigths', model_name='ast'):\n        '''\n        Метод сохранения весов модели по указанному адресу\n        '''\n        if model_name == 'ast':\n            state_dict = torch.load(path, map_location=self.device)\n            self.model.load_state_dict(state_dict, strict=False)\n        elif model_name == 'cnn':\n            #self.path1 = 'https://github.com/anuragkr90/weak_feature_extractor/blob/master/mx-h64-1024_0d3-1.17.pkl'\n            self.path_to_pretrained_weights = '../input/mypkl/mx-h64-1024_0d3-1.17.pkl'\n            #self.path_to_pretrained_weights = './params/pretrained_cnn_weights.pkl'\n            #if not os.path.exists(self.path_to_pretrained_weights):\n            #    wget.download(self.path1, out=self.path_to_pretrained_weights) \n\n            state_dict = torch.load(path, map_location=self.device)\n            #print(state_dict)\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n\n                if 'module.' in k:\n                    name = k[7:]\n                else:\n                    name = k\n                if k in ['layer19.0.weight', 'layer19.0.bias']:\n                    new_state_dict[name] = v\n            self.model.load_state_dict(new_state_dict, strict=False)\n\n    def download_weights(self):\n        '''\n        Метод для скачивания весов модели\n        '''\n        return FileLink(self.path_to_trained_weights)\n    \n    \n    def train(self, train_dataloader):\n        '''\n        Метод для обучения. Отображает оценки функции потерь и метрики в бар консоли.\n        train_dataloader - подгрузчик тренировочных данных по батчам\n        '''\n        self.model.train()\n        sum_loss = 0.\n        sum_metric_scores= 0.\n        cnt = 0\n        bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n        for _, (x, y) in bar:\n            x = x.to(self.device) # перебрасываем данные на GPU\n            y = y.to(self.device)\n\n            pred = self.model(x) \n            loss = self.criterion(pred, y) \n\n            loss.backward() # считаем градиенты\n            \n            self.optimizer.step() # делаем шаг в направлении минимума\n            self.optimizer.zero_grad()\n            self.scheduler.step() # уменьшаем lr\n            \n            pred = self.softmax(pred) # получаем вероятности\n            pred_ = pred.detach().to('cpu') # переносим данные из GPU на CPU\n            \n            y_ =  torch.argmax(y.detach().to('cpu').int(), dim=1) # считаем метрику\n            metric = self.metric(pred_, y_).item()\n\n            if cnt % 202 == 0: # \"обнуление\" счетчика и переменных сумм для отображения более актуальных средних значений loss и metric\n                cnt=1\n                sum_loss = loss.item()\n                sum_metric_scores = metric\n            else: # во всех иных случаях, высчитываем средние значения метрики\n                cnt+=1\n                sum_loss += loss.item()\n                sum_metric_scores += metric\n\n            bar.set_postfix(mean_train_loss=sum_loss/cnt, mean_train_metric=sum_metric_scores/cnt) # отображаем актуальные значения в баре\n            \n    def val(self, test_dataloader):\n        '''\n        Метод для валидации.Отображает оценки функции потерь и метрики в бар консоли.\n        train_dataloader - подгрузчик тестовых данных по батчам.\n        '''\n        bar = tqdm(enumerate(test_dataloader), total=len(test_dataloader))\n        cnt = 0\n        sum_loss = 0.\n        sum_metric_scores = 0.\n        cnt = 0\n        self.model.eval()\n        with torch.no_grad(): # отключаем вычисление градиента\n            for _, (x, y) in bar: \n                x = x.to(self.device)\n                y = y.to(self.device)\n                \n                pred = self.model(x) \n                loss = self.criterion(pred, y)\n\n                pred = self.softmax(pred) \n\n                pred_ = pred.detach().to('cpu')\n                y_ =  torch.argmax(y.detach().to('cpu').int(), dim=1)\n\n                metric = self.metric(pred_, y_).item()\n\n                cnt+=1\n                sum_loss += loss.item()\n                sum_metric_scores += metric\n                bar.set_postfix(mean_valid_loss=sum_loss/cnt, mean_valid_metric=sum_metric_scores/cnt)  \n        return sum_metric_scores/cnt\n    \n    def fit(self, train, test, save_best_weights=False):\n        '''\n        Метод обучения. Вохвращает последний результат метрики на валидации.\n        Если модель бьет рекорд по метрике - сохраняет ее веса.\n        train - тренировочные данные\n        test - тестовые данные.\n        '''\n        train_dataloader = torch.utils.data.DataLoader( # тренировочный подгрузчик данных\n            train,\n            batch_size=self.train_batch_size ,\n            shuffle=True,# перемешиваем данные\n            pin_memory=True,\n            drop_last=True\n        ) \n        \n        test_dataloader = torch.utils.data.DataLoader( # тестовый подгрузчик\n            test,\n            batch_size=self.test_batch_size,\n            shuffle=True,\n            pin_memory=True,\n            drop_last=True\n        )\n        self.best_score = 0.\n        for i in range(self.epoches):\n            self.train(train_dataloader)\n            res = self.val(test_dataloader)\n            if res > self.best_score:\n                self.best_score = res\n                if save_best_weights:\n                    self.save_weights(self.path_to_trained_weights) \n        return res\n    \n    def cross_validation_fit(self, dataset):\n        '''\n        Метод кросс-валидации. Выводит в консоль средний результат метрики по кросс-валидации\n        dataset - полный набор данных\n        '''\n        ds = DatasetECS50(PATHS , dataset.features, dataset.target)\n        max_num_fold = dataset.features['fold'].max()\n        self.save_weights() # сохраняем изначальные веса\n        result = 0.\n        for f in range(max_num_fold):\n            self.load_weights() # на каждом новом разбиении папок подгружаем изначальные веса\n            self.optimizer = self.type_of_optimizers[self.optimizer_name](self.model.parameters(), lr=self.lr)\n            train, test = ds.train_test_split(test_fold_num=f+1)\n            result += self.fit(train, test)\n        print('mean_{0}: '.format(self.metric_name), result/max_num_fold)","metadata":{"id":"DqlCo4lg_xTb","execution":{"iopub.status.busy":"2022-09-15T06:30:42.212184Z","iopub.execute_input":"2022-09-15T06:30:42.212443Z","iopub.status.idle":"2022-09-15T06:30:42.260237Z","shell.execute_reply.started":"2022-09-15T06:30:42.212419Z","shell.execute_reply":"2022-09-15T06:30:42.259243Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"clf = AudioClassifier(CONFIG, PATHS, CUSTOM_TOOLS)\nclf.fit(train, test, save_best_weights=True)","metadata":{"id":"aj3Xn1zH-66J","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Лучшие оценки, которых мне удалось достичь:\n\nМетрика - accuracy\n\ntrain - 1.0\n\nvalid - 0.925","metadata":{"id":"KbK5D2IvLcr2"}}]}